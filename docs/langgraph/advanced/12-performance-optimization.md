# æ•ˆèƒ½å„ªåŒ–

å»ºç«‹ç”Ÿç”¢ç´š Agent éœ€è¦æ³¨é‡æ•ˆèƒ½ã€‚æœ¬ç« ä»‹ç´¹å¸¸è¦‹å„ªåŒ–æŠ€å·§ã€‚

## å„ªåŒ–ç­–ç•¥

### 1. å¿«å–æ©Ÿåˆ¶

```python
from functools import lru_cache
import hashlib

class CachedLLM:
    def __init__(self):
        self.cache = {}

    def invoke(self, prompt: str) -> str:
        # ä½¿ç”¨ prompt hash ä½œç‚º key
        key = hashlib.md5(prompt.encode()).hexdigest()

        if key in self.cache:
            print("âœ… å¿«å–å‘½ä¸­")
            return self.cache[key]

        # å¯¦éš›èª¿ç”¨ LLM
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(model="gpt-4")
        result = llm.invoke(prompt).content

        self.cache[key] = result
        return result

# ä½¿ç”¨
cached_llm = CachedLLM()
response = cached_llm.invoke("åˆ†æå¸‚å ´è¶¨å‹¢")  # ç¬¬ä¸€æ¬¡èª¿ç”¨ API
response = cached_llm.invoke("åˆ†æå¸‚å ´è¶¨å‹¢")  # å¾å¿«å–å–å¾—
```

### 2. ä¸¦è¡Œè™•ç†

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List

def parallel_processing(items: List[str]) -> List[str]:
    """ä¸¦è¡Œè™•ç†å¤šå€‹é …ç›®"""

    def process_item(item):
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(model="gpt-4")
        return llm.invoke(f"è™•ç†: {item}").content

    results = []

    with ThreadPoolExecutor(max_workers=5) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        futures = {
            executor.submit(process_item, item): item
            for item in items
        }

        # æ”¶é›†çµæœ
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"è™•ç†å¤±æ•—: {e}")

    return results

# ä½¿ç”¨
items = ["é …ç›®1", "é …ç›®2", "é …ç›®3"]
results = parallel_processing(items)
```

### 3. æµå¼è¼¸å‡º

```python
from langchain_openai import ChatOpenAI

def streaming_response(prompt: str):
    """æµå¼è¼¸å‡º,å³æ™‚é¡¯ç¤ºçµæœ"""
    llm = ChatOpenAI(model="gpt-4", streaming=True)

    print("å›æ‡‰: ", end="", flush=True)
    for chunk in llm.stream(prompt):
        print(chunk.content, end="", flush=True)
    print()  # æ›è¡Œ

# ä½¿ç”¨
streaming_response("å¯«ä¸€é¦–è©©")
```

### 4. æ‰¹æ¬¡è™•ç†

```python
def batch_processing(queries: List[str], batch_size: int = 5):
    """æ‰¹æ¬¡è™•ç†æŸ¥è©¢"""
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-4")
    results = []

    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]

        # åˆä½µæŸ¥è©¢
        combined_prompt = "\n".join([
            f"{idx}. {q}"
            for idx, q in enumerate(batch, 1)
        ])

        # å–®æ¬¡èª¿ç”¨è™•ç†å¤šå€‹
        response = llm.invoke(f"å›ç­”ä»¥ä¸‹å•é¡Œ:\n{combined_prompt}")

        results.extend(response.content.split('\n'))

    return results
```

### 5. Token å„ªåŒ–

```python
def optimize_tokens(text: str, max_tokens: int = 1000) -> str:
    """å„ªåŒ– token ä½¿ç”¨"""
    from tiktoken import encoding_for_model

    enc = encoding_for_model("gpt-4")
    tokens = enc.encode(text)

    if len(tokens) <= max_tokens:
        return text

    # æˆªæ–·ä¸¦ä¿ç•™é‡è¦éƒ¨åˆ†
    truncated = tokens[:max_tokens]
    return enc.decode(truncated)

# ä½¿ç”¨
long_text = "..." * 10000
optimized = optimize_tokens(long_text, max_tokens=500)
```

### 6. æ¨¡å‹é¸æ“‡ç­–ç•¥

```python
def smart_model_selection(query: str, complexity: str):
    """æ ¹æ“šè¤‡é›œåº¦é¸æ“‡æ¨¡å‹"""
    from langchain_openai import ChatOpenAI

    if complexity == "simple":
        llm = ChatOpenAI(model="gpt-3.5-turbo")  # å¿«é€Ÿä¾¿å®œ
    elif complexity == "medium":
        llm = ChatOpenAI(model="gpt-4-turbo")
    else:
        llm = ChatOpenAI(model="gpt-4")  # æœ€å¼·ä½†æ…¢

    return llm.invoke(query).content
```

## å®Œæ•´å„ªåŒ–æ¡ˆä¾‹

```python
from typing import TypedDict
from langgraph.graph import StateGraph, END
from functools import lru_cache
import time

class OptimizedState(TypedDict):
    query: str
    result: str
    cache_hit: bool
    processing_time: float

# å¿«å–è£é£¾å™¨
@lru_cache(maxsize=100)
def cached_process(query: str) -> str:
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(model="gpt-3.5-turbo")  # ä½¿ç”¨è¼ƒå¿«çš„æ¨¡å‹
    return llm.invoke(query).content

def optimized_node(state: OptimizedState) -> OptimizedState:
    start = time.time()

    # å˜—è©¦å¾å¿«å–å–å¾—
    result = cached_process(state["query"])

    processing_time = time.time() - start

    return {
        "result": result,
        "processing_time": processing_time
    }

# å»ºç«‹å„ªåŒ–çš„ Agent
def create_optimized_agent():
    workflow = StateGraph(OptimizedState)
    workflow.add_node("process", optimized_node)
    workflow.set_entry_point("process")
    workflow.add_edge("process", END)
    return workflow.compile()

# ä½¿ç”¨ä¸¦æ¸¬è©¦
if __name__ == "__main__":
    agent = create_optimized_agent()

    # ç¬¬ä¸€æ¬¡æŸ¥è©¢
    result1 = agent.invoke({"query": "ä»€éº¼æ˜¯ AI?"})
    print(f"ç¬¬ä¸€æ¬¡: {result1['processing_time']:.2f}ç§’")

    # ç›¸åŒæŸ¥è©¢(å¿«å–)
    result2 = agent.invoke({"query": "ä»€éº¼æ˜¯ AI?"})
    print(f"ç¬¬äºŒæ¬¡(å¿«å–): {result2['processing_time']:.2f}ç§’")
```

## æ•ˆèƒ½ç›£æ§

```python
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start

        print(f"â±ï¸ {func.__name__}: {duration:.2f}ç§’")
        return result
    return wrapper

@monitor_performance
def slow_operation():
    time.sleep(2)
    return "å®Œæˆ"
```

## é‡é»æ•´ç†

### æ ¸å¿ƒå„ªåŒ–æŠ€è¡“
- **å¿«å–**: é¿å…é‡è¤‡è¨ˆç®—
- **ä¸¦è¡Œ**: åŒæ™‚è™•ç†å¤šå€‹ä»»å‹™
- **æµå¼**: å³æ™‚è¼¸å‡ºæå‡é«”é©—
- **æ‰¹æ¬¡**: æ¸›å°‘ API èª¿ç”¨æ¬¡æ•¸

### é¸æ“‡ç­–ç•¥
- ç°¡å–®ä»»å‹™ç”¨å¿«é€Ÿæ¨¡å‹
- è¤‡é›œä»»å‹™ç”¨å¼·å¤§æ¨¡å‹
- æ ¹æ“šé ç®—å’Œæ™‚é–“è¦æ±‚èª¿æ•´

### ç›£æ§æŒ‡æ¨™
- å›æ‡‰æ™‚é–“
- Token ä½¿ç”¨é‡
- å¿«å–å‘½ä¸­ç‡
- éŒ¯èª¤ç‡

## æœ€çµ‚å»ºè­°

1. **é‡æ¸¬å„ªå…ˆ**: å…ˆæ¸¬é‡å†å„ªåŒ–
2. **é€æ­¥æ”¹é€²**: ä¸€æ¬¡å„ªåŒ–ä¸€å€‹ç“¶é ¸
3. **å¹³è¡¡å–æ¨**: é€Ÿåº¦ vs å“è³ª vs æˆæœ¬
4. **æŒçºŒç›£æ§**: ç”Ÿç”¢ç’°å¢ƒæŒçºŒè¿½è¹¤

## ç·´ç¿’

1. ç‚ºä½ çš„ Agent æ·»åŠ å¿«å–æ©Ÿåˆ¶
2. å¯¦ä½œä¸¦è¡Œè™•ç†æå‡ååé‡
3. å»ºç«‹æ•ˆèƒ½ç›£æ§å„€è¡¨æ¿

---

## ğŸ‰ èª²ç¨‹å®Œæˆ!

æ­å–œæ‚¨å®Œæˆæ‰€æœ‰ 12 ç« çš„å­¸ç¿’!ç¾åœ¨æ‚¨å·²ç¶“æŒæ¡:

âœ… LangGraph æ ¸å¿ƒæ¦‚å¿µ
âœ… ç‹€æ…‹ç®¡ç†å’Œå·¥ä½œæµç¨‹è¨­è¨ˆ
âœ… å¯¦éš› Agent é–‹ç™¼
âœ… ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

### ä¸‹ä¸€æ­¥è¡Œå‹•

1. **å¯¦è¸å°ˆæ¡ˆ**: å»ºç«‹è‡ªå·±çš„ Agent æ‡‰ç”¨
2. **æ·±å…¥ç ”ç©¶**: æ¢ç´¢ LangGraph é€²éšåŠŸèƒ½
3. **ç¤¾ç¾¤äº¤æµ**: åˆ†äº«ç¶“é©—å’Œå­¸ç¿’æˆæœ
4. **æŒçºŒå­¸ç¿’**: é—œæ³¨ LangGraph æ›´æ–°

ç¥æ‚¨åœ¨ AI Agent é–‹ç™¼ä¹‹è·¯ä¸Šé †åˆ©!ğŸš€
