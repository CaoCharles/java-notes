{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - LangChain åŸºç¤\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "- âœ… ç†è§£ LangChain æ ¸å¿ƒæ¦‚å¿µèˆ‡æ¶æ§‹\n",
    "- âœ… æŒæ¡ PromptTemplate çš„ä½¿ç”¨æ–¹æ³•\n",
    "- âœ… å­¸ç¿’ LLM èˆ‡ ChatModel çš„å·®ç•°\n",
    "- âœ… å»ºç«‹ç¬¬ä¸€å€‹ Chain æ‡‰ç”¨\n",
    "- âœ… å¯¦ä½œåœ‹æ³°äººå£½ä¿å–®æŸ¥è©¢åŠ©ç†\n",
    "\n",
    "## â° é è¨ˆæ™‚é–“: 45 åˆ†é˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ç’°å¢ƒè¨­å®š\n",
    "\n",
    "### å®‰è£å¿…è¦å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ LangChain æ ¸å¿ƒå¥—ä»¶\n",
    "!pip install langchain==0.1.0\n",
    "!pip install langchain-openai==0.0.5\n",
    "!pip install langchain-core==0.1.10\n",
    "!pip install python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"OpenAI API Key: {'å·²è¨­å®š' if os.getenv('OPENAI_API_KEY') else 'æœªè¨­å®š'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LangChain æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "### 1.1 LangChain æ¶æ§‹æ¦‚è¦½\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         LangChain æ¶æ§‹å±¤æ¬¡              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Application Layer (ä½ çš„æ‡‰ç”¨)           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Chains (ä¸²æ¥å¤šå€‹çµ„ä»¶)                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Components:                            â”‚\n",
    "â”‚  - Prompts (æç¤ºè©æ¨¡æ¿)                 â”‚\n",
    "â”‚  - Models (LLM/ChatModel)              â”‚\n",
    "â”‚  - Output Parsers (è¼¸å‡ºè§£æ)           â”‚\n",
    "â”‚  - Memory (è¨˜æ†¶)                        â”‚\n",
    "â”‚  - Tools (å·¥å…·)                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  LangChain Core (æ ¸å¿ƒæŠ½è±¡)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ç¬¬ä¸€å€‹ LLM å‘¼å«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# åˆå§‹åŒ– ChatModel\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0.7,  # æ§åˆ¶å‰µæ„åº¦ (0-1)\n",
    "    max_tokens=500    # æœ€å¤§è¼¸å‡ºé•·åº¦\n",
    ")\n",
    "\n",
    "# ç°¡å–®å‘¼å«\n",
    "response = llm.invoke([HumanMessage(content=\"ä½ å¥½,è«‹ç°¡çŸ­ä»‹ç´¹åœ‹æ³°äººå£½\")])\n",
    "\n",
    "print(\"ğŸ¤– AI å›æ‡‰:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 LLM vs ChatModel å·®ç•°\n",
    "\n",
    "| ç‰¹æ€§ | LLM | ChatModel |\n",
    "|------|-----|----------|\n",
    "| è¼¸å…¥æ ¼å¼ | ç´”æ–‡å­—å­—ä¸² | Message ç‰©ä»¶åˆ—è¡¨ |\n",
    "| è¼¸å‡ºæ ¼å¼ | ç´”æ–‡å­—å­—ä¸² | AIMessage ç‰©ä»¶ |\n",
    "| æ”¯æ´è§’è‰² | ç„¡ | System/Human/AI |\n",
    "| å°è©±æ­·å² | éœ€æ‰‹å‹•ç®¡ç† | åŸç”Ÿæ”¯æ´ |\n",
    "| ç¯„ä¾‹æ¨¡å‹ | text-davinci-003 | gpt-4, gpt-3.5-turbo |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ä½¿ç”¨ ChatModel çš„å¤šè¼ªå°è©±\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯åœ‹æ³°äººå£½çš„å°ˆæ¥­å®¢æœäººå“¡,è«‹ç”¨å‹å–„ä¸”å°ˆæ¥­çš„æ…‹åº¦å›ç­”å®¢æˆ¶å•é¡Œã€‚\"),\n",
    "    HumanMessage(content=\"è«‹å•ä½ å€‘æœ‰å“ªäº›å£½éšªå•†å“?\"),\n",
    "    AIMessage(content=\"åœ‹æ³°äººå£½æä¾›å¤šå…ƒå£½éšªå•†å“,åŒ…æ‹¬çµ‚èº«å£½éšªã€å®šæœŸå£½éšªã€æŠ•è³‡å‹ä¿å–®ç­‰ã€‚\"),\n",
    "    HumanMessage(content=\"çµ‚èº«å£½éšªçš„ä¿éšœç¯„åœæ˜¯ä»€éº¼?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"ğŸ’¼ å®¢æœå›æ‡‰:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: PromptTemplate æç¤ºè©æ¨¡æ¿\n",
    "\n",
    "### 2.1 ç‚ºä»€éº¼éœ€è¦ PromptTemplate?\n",
    "\n",
    "âŒ **ä¸ä½¿ç”¨æ¨¡æ¿**:\n",
    "```python\n",
    "prompt = f\"è«‹æŸ¥è©¢ä¿å–®è™Ÿç¢¼ {policy_id} çš„è³‡è¨Š\"\n",
    "```\n",
    "å•é¡Œ: é›£ä»¥ç¶­è­·ã€å®¹æ˜“å‡ºéŒ¯ã€ç„¡æ³•é‡è¤‡ä½¿ç”¨\n",
    "\n",
    "âœ… **ä½¿ç”¨æ¨¡æ¿**:\n",
    "```python\n",
    "template = PromptTemplate(\n",
    "    template=\"è«‹æŸ¥è©¢ä¿å–®è™Ÿç¢¼ {policy_id} çš„è³‡è¨Š\",\n",
    "    input_variables=[\"policy_id\"]\n",
    ")\n",
    "```\n",
    "å„ªé»: å¯é‡è¤‡ä½¿ç”¨ã€åƒæ•¸åŒ–ã€æ˜“æ–¼æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 åŸºç¤ PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# å»ºç«‹ä¿å–®æŸ¥è©¢æ¨¡æ¿\n",
    "policy_template = PromptTemplate(\n",
    "    input_variables=[\"policy_id\", \"customer_name\"],\n",
    "    template=\"\"\"\n",
    "å®¢æˆ¶å§“å: {customer_name}\n",
    "ä¿å–®è™Ÿç¢¼: {policy_id}\n",
    "\n",
    "è«‹æŸ¥è©¢è©²ä¿å–®çš„ä»¥ä¸‹è³‡è¨Š:\n",
    "1. ä¿å–®ç‹€æ…‹\n",
    "2. ä¿éšœå…§å®¹\n",
    "3. ç¹³è²»ç‹€æ³\n",
    "4. å—ç›Šäººè³‡è¨Š\n",
    "\n",
    "è«‹ä»¥å°ˆæ¥­ä¸”æ˜“æ‡‚çš„æ–¹å¼èªªæ˜ã€‚\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨æ¨¡æ¿\n",
    "prompt = policy_template.format(\n",
    "    policy_id=\"CL202401001\",\n",
    "    customer_name=\"ç‹å°æ˜\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“„ ç”Ÿæˆçš„æç¤ºè©:\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ChatPromptTemplate (å°è©±æ¨¡æ¿)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# å»ºç«‹å°è©±æ¨¡æ¿\n",
    "system_template = \"ä½ æ˜¯åœ‹æ³°äººå£½çš„{role},æ“æœ‰{years}å¹´çš„å¾æ¥­ç¶“é©—ã€‚è«‹ç”¨å°ˆæ¥­ä¸”è¦ªåˆ‡çš„æ…‹åº¦å›ç­”å•é¡Œã€‚\"\n",
    "human_template = \"{customer_question}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template)\n",
    "])\n",
    "\n",
    "# æ ¼å¼åŒ–è¨Šæ¯\n",
    "messages = chat_prompt.format_messages(\n",
    "    role=\"è³‡æ·±ç†è²¡é¡§å•\",\n",
    "    years=10,\n",
    "    customer_question=\"è«‹å•æŠ•è³‡å‹ä¿å–®é©åˆæˆ‘é€™ç¨®ä¿å®ˆå‹æŠ•è³‡äººå—?\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ’¬ å°è©±è¨Šæ¯:\")\n",
    "for msg in messages:\n",
    "    print(f\"{msg.__class__.__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Chain éˆæ¥çµ„ä»¶\n",
    "\n",
    "### 3.1 ä»€éº¼æ˜¯ Chain?\n",
    "\n",
    "Chain æ˜¯ LangChain çš„æ ¸å¿ƒæ¦‚å¿µ,ç”¨æ–¼å°‡å¤šå€‹çµ„ä»¶ä¸²æ¥èµ·ä¾†:\n",
    "\n",
    "```\n",
    "Input â†’ PromptTemplate â†’ LLM â†’ OutputParser â†’ Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LLMChain åŸºç¤ç”¨æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# å»ºç«‹ Chain\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product_type\"],\n",
    "    template=\"è«‹ç”¨ 3 å€‹é‡é»èªªæ˜åœ‹æ³°äººå£½çš„{product_type}æœ‰å“ªäº›ç‰¹è‰²å’Œå„ªå‹¢ã€‚\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# åŸ·è¡Œ Chain\n",
    "result = chain.run(product_type=\"é†«ç™‚éšª\")\n",
    "print(\"ğŸ¥ é†«ç™‚éšªç‰¹è‰²:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ä½¿ç”¨ LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL æ˜¯æ›´ç¾ä»£çš„ Chain å¯«æ³•,ä½¿ç”¨ | é‹ç®—å­\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# å»ºç«‹ Chain: Prompt â†’ LLM â†’ Parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "result = chain.invoke({\"product_type\": \"å„²è“„éšª\"})\n",
    "print(\"ğŸ’° å„²è“„éšªç‰¹è‰²:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 SequentialChain é †åºéˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Chain 1: åˆ†æå®¢æˆ¶éœ€æ±‚\n",
    "analyze_prompt = PromptTemplate(\n",
    "    input_variables=[\"customer_info\"],\n",
    "    template=\"æ ¹æ“šä»¥ä¸‹å®¢æˆ¶è³‡è¨Š,åˆ†æå…¶ä¿éšªéœ€æ±‚:\\n{customer_info}\\n\\nåˆ†æçµæœ:\"\n",
    ")\n",
    "analyze_chain = LLMChain(llm=llm, prompt=analyze_prompt)\n",
    "\n",
    "# Chain 2: æ¨è–¦ç”¢å“\n",
    "recommend_prompt = PromptTemplate(\n",
    "    input_variables=[\"needs_analysis\"],\n",
    "    template=\"æ ¹æ“šéœ€æ±‚åˆ†æ:\\n{needs_analysis}\\n\\næ¨è–¦é©åˆçš„åœ‹æ³°äººå£½ä¿éšªå•†å“:\"\n",
    ")\n",
    "recommend_chain = LLMChain(llm=llm, prompt=recommend_prompt)\n",
    "\n",
    "# çµ„åˆæˆé †åºéˆ\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[analyze_chain, recommend_chain],\n",
    "    verbose=True  # é¡¯ç¤ºä¸­é–“éç¨‹\n",
    ")\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "customer_data = \"\"\"\n",
    "å§“å: é™³å°è¯\n",
    "å¹´é½¡: 35 æ­²\n",
    "è·æ¥­: å·¥ç¨‹å¸«\n",
    "å®¶åº­ç‹€æ³: å·²å©š,æœ‰ 2 å€‹å°å­© (5 æ­²ã€3 æ­²)\n",
    "å¹´æ”¶å…¥: 150 è¬\n",
    "ç¾æœ‰ä¿éšœ: åƒ…æœ‰å…¬å¸åœ˜ä¿\n",
    "\"\"\"\n",
    "\n",
    "result = overall_chain.run(customer_data)\n",
    "print(\"\\nğŸ“Š æœ€çµ‚æ¨è–¦:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Output Parsers è¼¸å‡ºè§£æ\n",
    "\n",
    "### 4.1 ç‚ºä»€éº¼éœ€è¦ Output Parser?\n",
    "\n",
    "LLM çš„è¼¸å‡ºé€šå¸¸æ˜¯ç´”æ–‡å­—,ä½†æˆ‘å€‘å¸¸éœ€è¦çµæ§‹åŒ–è³‡æ–™ (JSONã€List ç­‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# å»ºç«‹åˆ—è¡¨è§£æå™¨\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# å–å¾—æ ¼å¼æŒ‡ç¤º\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# å»ºç«‹æç¤ºè©\n",
    "prompt = PromptTemplate(\n",
    "    template=\"åˆ—å‡ºåœ‹æ³°äººå£½æœ€å—æ­¡è¿çš„ 5 å€‹ä¿éšªå•†å“ã€‚\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# å»ºç«‹ Chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "result = chain.invoke({})\n",
    "print(\"ğŸ“‹ ç†±é–€å•†å“åˆ—è¡¨:\")\n",
    "print(f\"é¡å‹: {type(result)}\")\n",
    "for i, product in enumerate(result, 1):\n",
    "    print(f\"{i}. {product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pydantic Output Parser (çµæ§‹åŒ–è¼¸å‡º)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# å®šç¾©ä¿å–®è³‡æ–™çµæ§‹\n",
    "class PolicyInfo(BaseModel):\n",
    "    policy_id: str = Field(description=\"ä¿å–®è™Ÿç¢¼\")\n",
    "    policy_type: str = Field(description=\"ä¿å–®é¡å‹\")\n",
    "    status: str = Field(description=\"ä¿å–®ç‹€æ…‹\")\n",
    "    premium: int = Field(description=\"å¹´ç¹³ä¿è²»\")\n",
    "    coverage: List[str] = Field(description=\"ä¿éšœé …ç›®åˆ—è¡¨\")\n",
    "\n",
    "# å»ºç«‹è§£æå™¨\n",
    "parser = PydanticOutputParser(pydantic_object=PolicyInfo)\n",
    "\n",
    "# å»ºç«‹æç¤ºè©\n",
    "prompt = PromptTemplate(\n",
    "    template=\"è«‹ç”Ÿæˆä¸€å€‹ç¯„ä¾‹ä¿å–®è³‡æ–™ã€‚\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "chain = prompt | llm | parser\n",
    "result = chain.invoke({})\n",
    "\n",
    "print(\"ğŸ“„ ä¿å–®è³‡æ–™:\")\n",
    "print(f\"é¡å‹: {type(result)}\")\n",
    "print(f\"ä¿å–®è™Ÿç¢¼: {result.policy_id}\")\n",
    "print(f\"ä¿å–®é¡å‹: {result.policy_type}\")\n",
    "print(f\"ä¿å–®ç‹€æ…‹: {result.status}\")\n",
    "print(f\"å¹´ç¹³ä¿è²»: ${result.premium:,}\")\n",
    "print(f\"ä¿éšœé …ç›®: {', '.join(result.coverage)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: å¯¦æˆ°æ¡ˆä¾‹ - åœ‹æ³°äººå£½ä¿å–®æŸ¥è©¢åŠ©ç†\n",
    "\n",
    "### æ•´åˆæ‰€æœ‰å­¸åˆ°çš„æŠ€è¡“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class PolicyQueryResult(BaseModel):\n",
    "    \"\"\"ä¿å–®æŸ¥è©¢çµæœ\"\"\"\n",
    "    policy_id: str = Field(description=\"ä¿å–®è™Ÿç¢¼\")\n",
    "    customer_name: str = Field(description=\"å®¢æˆ¶å§“å\")\n",
    "    status: str = Field(description=\"ä¿å–®ç‹€æ…‹:æœ‰æ•ˆ/å¤±æ•ˆ/ç­‰å¾…æœŸ\")\n",
    "    next_payment_date: str = Field(description=\"ä¸‹æ¬¡ç¹³è²»æ—¥æœŸ (YYYY-MM-DD)\")\n",
    "    summary: str = Field(description=\"ä¿å–®æ‘˜è¦èªªæ˜\")\n",
    "    suggestions: List[str] = Field(description=\"å®¢æœå»ºè­°åˆ—è¡¨\")\n",
    "\n",
    "# å»ºç«‹è§£æå™¨\n",
    "query_parser = PydanticOutputParser(pydantic_object=PolicyQueryResult)\n",
    "\n",
    "# å»ºç«‹ç³»çµ±æç¤ºè©\n",
    "system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"\n",
    "ä½ æ˜¯åœ‹æ³°äººå£½çš„æ™ºèƒ½å®¢æœç³»çµ±ã€‚\n",
    "ç•¶å®¢æˆ¶æŸ¥è©¢ä¿å–®æ™‚,ä½ éœ€è¦:\n",
    "1. ç¢ºèªå®¢æˆ¶èº«åˆ†å’Œä¿å–®è™Ÿç¢¼\n",
    "2. æä¾›ä¿å–®ç‹€æ…‹è³‡è¨Š\n",
    "3. èªªæ˜ä¸‹æ¬¡ç¹³è²»æ—¥æœŸ\n",
    "4. æä¾›å°ˆæ¥­å»ºè­°\n",
    "\n",
    "è«‹å‹™å¿…ä¿æŒå°ˆæ¥­ã€æº–ç¢ºã€è¦ªåˆ‡çš„æ…‹åº¦ã€‚\n",
    "\"\"\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "å®¢æˆ¶æŸ¥è©¢è³‡è¨Š:\n",
    "ä¿å–®è™Ÿç¢¼: {policy_id}\n",
    "å®¢æˆ¶å§“å: {customer_name}\n",
    "æŸ¥è©¢æ™‚é–“: {query_time}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# å»ºç«‹æŸ¥è©¢ Chain\n",
    "policy_query_chain = (\n",
    "    system_prompt \n",
    "    | llm \n",
    "    | query_parser\n",
    ")\n",
    "\n",
    "# åŸ·è¡ŒæŸ¥è©¢\n",
    "result = policy_query_chain.invoke({\n",
    "    \"policy_id\": \"CL202401001\",\n",
    "    \"customer_name\": \"ç‹å°æ˜\",\n",
    "    \"query_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"format_instructions\": query_parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¢ åœ‹æ³°äººå£½ä¿å–®æŸ¥è©¢ç³»çµ±\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“‹ ä¿å–®è™Ÿç¢¼: {result.policy_id}\")\n",
    "print(f\"ğŸ‘¤ å®¢æˆ¶å§“å: {result.customer_name}\")\n",
    "print(f\"âœ… ä¿å–®ç‹€æ…‹: {result.status}\")\n",
    "print(f\"ğŸ“… ä¸‹æ¬¡ç¹³è²»æ—¥: {result.next_payment_date}\")\n",
    "print(f\"\\nğŸ“ ä¿å–®æ‘˜è¦:\\n{result.summary}\")\n",
    "print(f\"\\nğŸ’¡ å°ˆæ¥­å»ºè­°:\")\n",
    "for i, suggestion in enumerate(result.suggestions, 1):\n",
    "    print(f\"   {i}. {suggestion}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ç·´ç¿’é¡Œ\n",
    "\n",
    "### ç·´ç¿’ 1: å»ºç«‹ç†è³ æŸ¥è©¢ç³»çµ±\n",
    "\n",
    "**éœ€æ±‚**:\n",
    "1. å®šç¾© ClaimInfo è³‡æ–™çµæ§‹ (ç†è³ ç·¨è™Ÿã€ç‹€æ…‹ã€é‡‘é¡ã€è™•ç†é€²åº¦)\n",
    "2. å»ºç«‹ PromptTemplate\n",
    "3. ä½¿ç”¨ Pydantic Parser\n",
    "4. å»ºç«‹å®Œæ•´çš„æŸ¥è©¢ Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: åœ¨æ­¤å¯¦ä½œç†è³ æŸ¥è©¢ç³»çµ±\n",
    "\n",
    "class ClaimInfo(BaseModel):\n",
    "    # TODO: å®šç¾©æ¬„ä½\n",
    "    pass\n",
    "\n",
    "# TODO: å»ºç«‹ Parser å’Œ Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: å¤šæ­¥é©Ÿç”¢å“æ¨è–¦\n",
    "\n",
    "**éœ€æ±‚**:\n",
    "1. Step 1: åˆ†æå®¢æˆ¶å¹´é½¡ã€æ”¶å…¥ã€å®¶åº­ç‹€æ³\n",
    "2. Step 2: è©•ä¼°é¢¨éšªæ‰¿å—åº¦\n",
    "3. Step 3: æ¨è–¦ 3 å€‹é©åˆçš„ä¿éšªå•†å“\n",
    "4. ä½¿ç”¨ SequentialChain ä¸²æ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å»ºç«‹å¤šæ­¥é©Ÿæ¨è–¦ç³»çµ±\n",
    "\n",
    "# Step 1: å®¢æˆ¶åˆ†æ\n",
    "# Step 2: é¢¨éšªè©•ä¼°  \n",
    "# Step 3: ç”¢å“æ¨è–¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 3: ä¿è²»è©¦ç®—å·¥å…·\n",
    "\n",
    "**éœ€æ±‚**:\n",
    "1. è¼¸å…¥: å¹´é½¡ã€æ€§åˆ¥ã€ä¿é¡ã€ä¿éšœå¹´æœŸ\n",
    "2. è¼¸å‡ºçµæ§‹åŒ–çš„è©¦ç®—çµæœ (å¹´ç¹³/æœˆç¹³/ç¸½ä¿è²»)\n",
    "3. æä¾›ç¹³è²»å»ºè­°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å»ºç«‹ä¿è²»è©¦ç®—å·¥å…·\n",
    "\n",
    "class PremiumCalculation(BaseModel):\n",
    "    # TODO: å®šç¾©è©¦ç®—çµæœçµæ§‹\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… ç·´ç¿’è§£ç­”\n",
    "\n",
    "### ç·´ç¿’ 1 è§£ç­”: ç†è³ æŸ¥è©¢ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimInfoSolution(BaseModel):\n",
    "    \"\"\"ç†è³ è³‡è¨Š\"\"\"\n",
    "    claim_id: str = Field(description=\"ç†è³ ç·¨è™Ÿ\")\n",
    "    policy_id: str = Field(description=\"ä¿å–®è™Ÿç¢¼\")\n",
    "    claim_type: str = Field(description=\"ç†è³ é¡å‹:é†«ç™‚/æ„å¤–/èº«æ•…\")\n",
    "    claim_amount: int = Field(description=\"ç†è³ é‡‘é¡\")\n",
    "    status: str = Field(description=\"è™•ç†ç‹€æ…‹:å¯©æ ¸ä¸­/å·²æ ¸å‡†/å·²æ’¥æ¬¾/å·²æ‹’çµ•\")\n",
    "    progress_percentage: int = Field(description=\"è™•ç†é€²åº¦ç™¾åˆ†æ¯” 0-100\")\n",
    "    estimated_completion: str = Field(description=\"é è¨ˆå®Œæˆæ—¥æœŸ\")\n",
    "    required_documents: List[str] = Field(description=\"éœ€è£œä»¶åˆ—è¡¨\")\n",
    "\n",
    "# å»ºç«‹è§£æå™¨\n",
    "claim_parser = PydanticOutputParser(pydantic_object=ClaimInfoSolution)\n",
    "\n",
    "# å»ºç«‹æç¤ºè©\n",
    "claim_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ æ˜¯åœ‹æ³°äººå£½ç†è³ éƒ¨é–€çš„å®¢æœç³»çµ±,è«‹æä¾›æº–ç¢ºçš„ç†è³ é€²åº¦è³‡è¨Šã€‚\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "æŸ¥è©¢ç†è³ æ¡ˆä»¶:\n",
    "ç†è³ ç·¨è™Ÿ: {claim_id}\n",
    "å®¢æˆ¶å§“å: {customer_name}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# å»ºç«‹ Chain\n",
    "claim_chain = claim_prompt | llm | claim_parser\n",
    "\n",
    "# åŸ·è¡ŒæŸ¥è©¢\n",
    "claim_result = claim_chain.invoke({\n",
    "    \"claim_id\": \"CLM20240315001\",\n",
    "    \"customer_name\": \"æå°è¯\",\n",
    "    \"format_instructions\": claim_parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ¥ ç†è³ æŸ¥è©¢çµæœ\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ç†è³ ç·¨è™Ÿ: {claim_result.claim_id}\")\n",
    "print(f\"ä¿å–®è™Ÿç¢¼: {claim_result.policy_id}\")\n",
    "print(f\"ç†è³ é¡å‹: {claim_result.claim_type}\")\n",
    "print(f\"ç†è³ é‡‘é¡: ${claim_result.claim_amount:,}\")\n",
    "print(f\"è™•ç†ç‹€æ…‹: {claim_result.status}\")\n",
    "print(f\"è™•ç†é€²åº¦: {claim_result.progress_percentage}%\")\n",
    "print(f\"é è¨ˆå®Œæˆ: {claim_result.estimated_completion}\")\n",
    "if claim_result.required_documents:\n",
    "    print(f\"\\nğŸ“ éœ€è£œä»¶:\")\n",
    "    for doc in claim_result.required_documents:\n",
    "        print(f\"  - {doc}\")\n",
    "else:\n",
    "    print(\"\\nâœ… ç„¡éœ€è£œä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2 è§£ç­”: å¤šæ­¥é©Ÿç”¢å“æ¨è–¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: å®¢æˆ¶åˆ†æ Chain\n",
    "analysis_prompt = PromptTemplate(\n",
    "    input_variables=[\"customer_profile\"],\n",
    "    template=\"\"\"\n",
    "åˆ†æä»¥ä¸‹å®¢æˆ¶è³‡æ–™,è©•ä¼°å…¶ç”Ÿæ¶¯éšæ®µå’Œä¿éšªéœ€æ±‚:\n",
    "\n",
    "{customer_profile}\n",
    "\n",
    "è«‹åˆ†æ:\n",
    "1. ç”Ÿæ¶¯éšæ®µ\n",
    "2. ä¸»è¦é¢¨éšªç¼ºå£\n",
    "3. ä¿éšœå„ªå…ˆé †åº\n",
    "\"\"\"\n",
    ")\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: é¢¨éšªè©•ä¼° Chain\n",
    "risk_prompt = PromptTemplate(\n",
    "    input_variables=[\"analysis\"],\n",
    "    template=\"\"\"\n",
    "æ ¹æ“šå®¢æˆ¶åˆ†æçµæœ:\n",
    "{analysis}\n",
    "\n",
    "è©•ä¼°å®¢æˆ¶çš„é¢¨éšªæ‰¿å—åº¦ (ä¿å®ˆ/ç©©å¥/ç©æ¥µ),ä¸¦èªªæ˜ç†ç”±ã€‚\n",
    "\"\"\"\n",
    ")\n",
    "risk_chain = risk_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 3: ç”¢å“æ¨è–¦ Chain\n",
    "recommend_prompt = PromptTemplate(\n",
    "    input_variables=[\"risk_assessment\"],\n",
    "    template=\"\"\"\n",
    "åŸºæ–¼é¢¨éšªè©•ä¼°:\n",
    "{risk_assessment}\n",
    "\n",
    "æ¨è–¦ 3 å€‹åœ‹æ³°äººå£½é©åˆçš„ä¿éšªå•†å“,ä¸¦èªªæ˜æ¨è–¦ç†ç”±ã€‚\n",
    "\"\"\"\n",
    ")\n",
    "recommend_chain = recommend_prompt | llm | StrOutputParser()\n",
    "\n",
    "# çµ„åˆæˆé †åºéˆ (ä½¿ç”¨ LCEL)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "full_chain = (\n",
    "    {\"customer_profile\": RunnablePassthrough()}\n",
    "    | analysis_chain\n",
    "    | {\"analysis\": RunnablePassthrough()}\n",
    "    | risk_chain\n",
    "    | {\"risk_assessment\": RunnablePassthrough()}\n",
    "    | recommend_chain\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦\n",
    "customer = \"\"\"\n",
    "å§“å: å¼µç¾ç²\n",
    "å¹´é½¡: 28 æ­²\n",
    "è·æ¥­: è»Ÿé«”å·¥ç¨‹å¸«\n",
    "å©šå§»: æœªå©š\n",
    "å¹´æ”¶å…¥: 120 è¬\n",
    "å­˜æ¬¾: 80 è¬\n",
    "æŠ•è³‡ç¶“é©—: æœ‰è³¼è²·åŸºé‡‘ç¶“é©—\n",
    "ç¾æœ‰ä¿éšœ: å…¬å¸åœ˜ä¿ + æ„å¤–éšª\n",
    "\"\"\"\n",
    "\n",
    "result = full_chain.invoke(customer)\n",
    "print(\"\\nğŸ’¼ ç”¢å“æ¨è–¦çµæœ\")\n",
    "print(\"=\"*60)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 3 è§£ç­”: ä¿è²»è©¦ç®—å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PremiumCalculationSolution(BaseModel):\n",
    "    \"\"\"ä¿è²»è©¦ç®—çµæœ\"\"\"\n",
    "    product_name: str = Field(description=\"å•†å“åç¨±\")\n",
    "    coverage_amount: int = Field(description=\"ä¿é¡\")\n",
    "    coverage_years: int = Field(description=\"ä¿éšœå¹´æœŸ\")\n",
    "    annual_premium: int = Field(description=\"å¹´ç¹³ä¿è²»\")\n",
    "    monthly_premium: int = Field(description=\"æœˆç¹³ä¿è²»\")\n",
    "    total_premium: int = Field(description=\"ç¸½ç¹³ä¿è²»\")\n",
    "    payment_suggestion: str = Field(description=\"ç¹³è²»å»ºè­°\")\n",
    "    benefit_summary: List[str] = Field(description=\"ä¿éšœæ‘˜è¦\")\n",
    "\n",
    "# å»ºç«‹è§£æå™¨\n",
    "premium_parser = PydanticOutputParser(pydantic_object=PremiumCalculationSolution)\n",
    "\n",
    "# å»ºç«‹æç¤ºè©\n",
    "premium_prompt = PromptTemplate(\n",
    "    input_variables=[\"age\", \"gender\", \"coverage\", \"years\"],\n",
    "    template=\"\"\"\n",
    "è«‹ç‚ºä»¥ä¸‹æ¢ä»¶è©¦ç®—åœ‹æ³°äººå£½çµ‚èº«å£½éšªä¿è²»:\n",
    "\n",
    "å¹´é½¡: {age} æ­²\n",
    "æ€§åˆ¥: {gender}\n",
    "ä¿é¡: ${coverage:,}\n",
    "ä¿éšœå¹´æœŸ: {years} å¹´\n",
    "\n",
    "è«‹æä¾›å®Œæ•´çš„ä¿è²»è©¦ç®—çµæœå’Œç¹³è²»å»ºè­°ã€‚\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    partial_variables={\"format_instructions\": premium_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# å»ºç«‹ Chain\n",
    "premium_chain = premium_prompt | llm | premium_parser\n",
    "\n",
    "# åŸ·è¡Œè©¦ç®—\n",
    "calc_result = premium_chain.invoke({\n",
    "    \"age\": 30,\n",
    "    \"gender\": \"ç”·\",\n",
    "    \"coverage\": 2000000,\n",
    "    \"years\": 20\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ’° ä¿è²»è©¦ç®—çµæœ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"å•†å“: {calc_result.product_name}\")\n",
    "print(f\"ä¿é¡: ${calc_result.coverage_amount:,}\")\n",
    "print(f\"ä¿éšœå¹´æœŸ: {calc_result.coverage_years} å¹´\")\n",
    "print(f\"\\nğŸ’µ ä¿è²»è³‡è¨Š:\")\n",
    "print(f\"  å¹´ç¹³: ${calc_result.annual_premium:,}\")\n",
    "print(f\"  æœˆç¹³: ${calc_result.monthly_premium:,}\")\n",
    "print(f\"  ç¸½ç¹³: ${calc_result.total_premium:,}\")\n",
    "print(f\"\\nğŸ’¡ ç¹³è²»å»ºè­°:\\n{calc_result.payment_suggestion}\")\n",
    "print(f\"\\nâœ… ä¿éšœå…§å®¹:\")\n",
    "for benefit in calc_result.benefit_summary:\n",
    "    print(f\"  â€¢ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ å­¸ç¿’æª¢æŸ¥æ¸…å–®\n",
    "\n",
    "å®Œæˆæœ¬ Notebook å¾Œ,ä½ æ‡‰è©²èƒ½å¤ :\n",
    "\n",
    "- [ ] ç†è§£ LangChain çš„æ ¸å¿ƒæ¶æ§‹\n",
    "- [ ] å€åˆ† LLM å’Œ ChatModel çš„å·®ç•°\n",
    "- [ ] ä½¿ç”¨ PromptTemplate å»ºç«‹å¯é‡è¤‡ä½¿ç”¨çš„æç¤ºè©\n",
    "- [ ] ä½¿ç”¨ ChatPromptTemplate å»ºç«‹å°è©±æ¨¡æ¿\n",
    "- [ ] ç†è§£ Chain çš„æ¦‚å¿µä¸¦å»ºç«‹ç°¡å–®çš„ LLMChain\n",
    "- [ ] ä½¿ç”¨ LCEL (LangChain Expression Language)\n",
    "- [ ] ä½¿ç”¨ SequentialChain å»ºç«‹å¤šæ­¥é©Ÿæµç¨‹\n",
    "- [ ] ä½¿ç”¨ Output Parsers å–å¾—çµæ§‹åŒ–è¼¸å‡º\n",
    "- [ ] ä½¿ç”¨ Pydantic å®šç¾©è¤‡é›œçš„è¼¸å‡ºçµæ§‹\n",
    "- [ ] æ•´åˆæ‰€æœ‰æŠ€è¡“å»ºç«‹å¯¦ç”¨çš„æ‡‰ç”¨\n",
    "\n",
    "## ğŸ“ é‡é»å›é¡§\n",
    "\n",
    "### LangChain æ ¸å¿ƒæ¦‚å¿µ\n",
    "1. **Components**: Prompts, Models, Parsers, Memory, Tools\n",
    "2. **Chains**: ä¸²æ¥å¤šå€‹çµ„ä»¶å½¢æˆå®Œæ•´æµç¨‹\n",
    "3. **LCEL**: ä½¿ç”¨ `|` é‹ç®—å­çš„ç¾ä»£åŒ– Chain èªæ³•\n",
    "\n",
    "### æœ€ä½³å¯¦è¸\n",
    "1. ä½¿ç”¨ PromptTemplate ç®¡ç†æç¤ºè©\n",
    "2. ä½¿ç”¨ Pydantic å®šç¾©çµæ§‹åŒ–è¼¸å‡º\n",
    "3. ä½¿ç”¨ ChatModel è€Œé LLM (æ”¯æ´è§’è‰²å’Œå°è©±æ­·å²)\n",
    "4. ä½¿ç”¨ LCEL å»ºç«‹å¯è®€æ€§é«˜çš„ Chain\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "ç¹¼çºŒå‰å¾€ `02-langchain-agents.ipynb` å­¸ç¿’å¦‚ä½•å»ºç«‹ LangChain Agent ğŸ‘‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
